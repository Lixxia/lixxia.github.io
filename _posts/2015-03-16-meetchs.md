---
layout: post
title: Planning to Meet Charleston
date: 2015-03-16 1:26PM
---

Of the [list](http://www.charlestondigitalcorridor.com/talent/user-groups/) of meetups they were either infrequent or uninteresting to me. Instead I'm planning to attend either [Tech After Five](http://techafterfive.com/ta5/charleston-sc/) or [Women in Tech](http://www.meetup.com/Charleston-Women-In-Tech/).

Given that there isn't much to say on this topic until I actually attend a meeting, I thought I'd talk about an interesting article I read recently.

# [Stanford Study On Programming Styles](https://ed.stanford.edu/news/stanford-study-shows-success-different-learning-styles-computer-science-class)

This article outlines the results of a joint research effort between Stanford’s Computer Science Department and its Graduate School of Education. Instead of focusing on students before they learn and after they learn, they focused on the period in between. This was done by utilizing machine learning to analyze different coding patterns used by students. 

The students were classified in three different groups (alpha, beta and gamma), each with a different style. It was also noted that there was a surprising amount of students who fell in between the styles, those who utilized different, but effective approaches to programming. 

>Alpha students, explained co-author Piech, moved efficiently from one point to another as they wrote code, creating a streamlined program. Beta and gamma students, on the other hand, wrote themselves into so-called “sink states” in which their programs slammed into dead ends from which they had to back out before they could resume progress.

However, the point that struck me as most important was this:

>In addition, this kind of process-based assessment can be more effective in determining what students actually have learned, Blikstein said. “Some students know the material – sometimes better than the typical ‘A’ students — but they’re not good at taking tests,” he said. “Testing is normally a poor way of determining if students have learned something or not.”

It's easy to test students. A lot less effort for the teachers, and to some, less effort for the students. A quick assessment, a couple of questions usually involving a regurgitation of memorized knowledge as opposed to thoughtful input. This is not the fault of tests, it's the fault of how we evaluate progress. 

Changing the system of standardized instruction is not an easy task. Some would be happy to leave it the way it is. However it should not be so. Students who are poor test takers but have a great grasp of the knowledge should not be left to struggle, and evaluating intelligence and progress based on a simple test is not the way to go. 

>Rather than so-called “teaching to the test,” automated data collection and analysis techniques can lead to new opportunities for project-based learning that focuses on the student.  

>Our goal is [...] to use machine learning to [...] open it up by putting more project-based, work into computer science classrooms

I've seen so many students do well on tests, but fail to apply the concepts in a project-based scenario. This illustrates a serious lack of true understanding, and to me shows a very shallow grasp of what's needed. The article phrases it well: "teaching to test". But teacher's aren't just doing that, students are "learning to test". I myself fall victim to this, given that such weight is put on our grades. We can't learn for knowledge's sake, we have to learn what's relevant on the test. It becomes a task to learn beyond that, and although I think the pursuit is worth it, not many students do. 